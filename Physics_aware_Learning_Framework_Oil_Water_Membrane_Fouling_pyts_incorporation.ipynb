{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gay4R0vXwmU"
      },
      "outputs": [],
      "source": [
        "# ===================== FINAL curve→meta (pyts-style) wrap-up =====================\n",
        "# MiniRocket + shape stats, grouped CV; categorical + numeric(3-bin); time-window importance\n",
        "# Assumes: `dfm` is in memory with columns:\n",
        "#   - y_0..y_59 (normalized flux at 60 time points)\n",
        "#   - article_id (or at least curve_id)\n",
        "#   - metadata columns: material, oil_type, droplet_um, p_bar, cross_flow_ms, porosity, pore_nm, jw_lmh, oil_ppm, salt_gl, temp_c\n",
        "# Outputs: ./pyts_final/ and pyts_final.zip with:\n",
        "#   - curve_to_meta_results.csv\n",
        "#   - time_window_importance.csv (if any targets)\n",
        "#   - one example bar plot of time-window importance\n",
        "\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
        "from sktime.transformations.panel.rocket import MiniRocketMultivariate\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "import shutil, os, warnings\n",
        "\n",
        "RNG = 42\n",
        "OUT = Path(\"pyts_final\")\n",
        "OUT.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "assert 'dfm' in globals(), \"Need `dfm` loaded (merged meta + y_0..y_59).\"\n",
        "\n",
        "# --------- shape stats on curves ----------\n",
        "def shape_feats(A: np.ndarray) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Simple shape descriptors from normalized curves A (n_samples, L):\n",
        "      - total AUC\n",
        "      - slope + mean for each third of the curve\n",
        "      - curvature-like mid metric\n",
        "      - last value and global std\n",
        "    \"\"\"\n",
        "    n, L = A.shape\n",
        "    t = np.linspace(0, 1, L)\n",
        "    thirds = np.array_split(np.arange(L), 3)\n",
        "    out = {}\n",
        "    out[\"auc\"] = np.trapz(A, x=t, axis=1)\n",
        "    for k, idxs in enumerate(thirds, 1):\n",
        "        x = t[idxs] - t[idxs].mean()\n",
        "        denom = (x**2).sum() + 1e-12\n",
        "        # per-curve slope in that segment\n",
        "        slope = ((A[:, idxs] - A[:, idxs].mean(axis=1, keepdims=True)) @ x) / denom\n",
        "        out[f\"slope_{k}\"] = slope\n",
        "        out[f\"mean_{k}\"]  = A[:, idxs].mean(axis=1)\n",
        "    # “curvature” in the middle third vs outer thirds\n",
        "    out[\"curv_mid\"] = out[\"slope_2\"] - 0.5*(out[\"slope_1\"] + out[\"slope_3\"])\n",
        "    out[\"last\"] = A[:, -1]\n",
        "    out[\"std\"]  = A.std(axis=1)\n",
        "    return pd.DataFrame(out)\n",
        "\n",
        "# --------- build curves (Yshape, Ydiff) from y_0..y_59 ----------\n",
        "def build_curves(df: pd.DataFrame, L: int = 60):\n",
        "    \"\"\"\n",
        "    From df[y_0..y_59] build:\n",
        "      - Yshape: monotone, shape-normalized curves\n",
        "      - Ydiff:  first differences (with padding)\n",
        "    Returns (ycols, Yshape, Ydiff)\n",
        "    \"\"\"\n",
        "    ycols = sorted(\n",
        "        [c for c in df.columns if str(c).startswith(\"y_\")],\n",
        "        key=lambda s: int(str(s).split(\"_\")[1])\n",
        "    )[:L]\n",
        "    if len(ycols) < L:\n",
        "        warnings.warn(f\"Only {len(ycols)} y_* columns found, expected {L}.\")\n",
        "    Yraw = df[ycols].to_numpy(float)\n",
        "\n",
        "    # physics-aware smoothing + shape normalization\n",
        "    Ymono  = np.minimum.accumulate(Yraw, axis=1)              # enforce monotone decay\n",
        "    y0     = np.maximum(Ymono[:, [0]], 1e-9)                  # avoid division by zero\n",
        "    Yshape = Ymono / y0                                       # normalize by initial flux\n",
        "    Ydiff  = np.c_[np.diff(Ymono, axis=1),\n",
        "                   np.zeros((len(Ymono), 1))]                 # pad to length L\n",
        "\n",
        "    return ycols, Yshape, Ydiff\n",
        "\n",
        "# --------- OOF MiniRocket features ----------\n",
        "def oof_minirocket(Xpanel: np.ndarray, groups: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Out-of-fold MiniRocket features (then SVD + scaling).\n",
        "    Xpanel shape: (n_samples, n_channels, L)  (here 2 channels: Yshape, Ydiff)\n",
        "    Returns Xstd: (n_samples, n_components)\n",
        "    \"\"\"\n",
        "    gkf = GroupKFold(n_splits=min(5, max(3, len(np.unique(groups)))))\n",
        "    oof = None\n",
        "    for tr, te in gkf.split(Xpanel, groups=groups):\n",
        "        mr = MiniRocketMultivariate(random_state=RNG)\n",
        "        mr.fit(Xpanel[tr])\n",
        "        Xtr = mr.transform(Xpanel[tr])\n",
        "        Xte = mr.transform(Xpanel[te])\n",
        "        if oof is None:\n",
        "            oof = np.zeros((Xpanel.shape[0], Xtr.shape[1]), dtype=float)\n",
        "        oof[tr] = Xtr\n",
        "        oof[te] = Xte\n",
        "\n",
        "    # compress with SVD + scale (no centering)\n",
        "    n_comp = max(2, min(300, oof.shape[1]-1))\n",
        "    svd = TruncatedSVD(n_components=n_comp, random_state=RNG)\n",
        "    Xsvd = svd.fit_transform(oof)\n",
        "    Xstd = StandardScaler(with_mean=False).fit_transform(Xsvd)\n",
        "    return Xstd\n",
        "\n",
        "# --------- grouped CV classifier on stacked features ----------\n",
        "def run_categorical(df_sub, y, label_name, groups, Yshape, Ydiff, Xmr_oof):\n",
        "    \"\"\"\n",
        "    Grouped CV for a categorical target:\n",
        "      - shape_feats(Yshape) + MiniRocket OOF features\n",
        "      - LogisticRegression with class_weight='balanced'\n",
        "    Returns F1_macro and accuracy.\n",
        "    \"\"\"\n",
        "    gkf = GroupKFold(n_splits=min(5, max(3, len(np.unique(groups)))))\n",
        "\n",
        "    oof_pred = np.full(len(df_sub), -1, dtype=int)\n",
        "    f1s, accs = [], []\n",
        "\n",
        "    for tr, te in gkf.split(Yshape, groups=groups):\n",
        "        # shape features\n",
        "        shp_tr = shape_feats(Yshape[tr]).to_numpy(float)\n",
        "        shp_te = shape_feats(Yshape[te]).to_numpy(float)\n",
        "\n",
        "        # scalers for shape + MiniRocket parts\n",
        "        ss_shp = StandardScaler(with_mean=True).fit(shp_tr)\n",
        "        ss_mr  = StandardScaler(with_mean=False).fit(Xmr_oof[tr])\n",
        "\n",
        "        Xtr = hstack([\n",
        "            csr_matrix(ss_shp.transform(shp_tr)),\n",
        "            csr_matrix(ss_mr.transform(Xmr_oof[tr]))\n",
        "        ])\n",
        "        Xte = hstack([\n",
        "            csr_matrix(ss_shp.transform(shp_te)),\n",
        "            csr_matrix(ss_mr.transform(Xmr_oof[te]))\n",
        "        ])\n",
        "\n",
        "        clf = LogisticRegression(\n",
        "            max_iter=4000,\n",
        "            class_weight='balanced',\n",
        "            multi_class='auto',\n",
        "            solver='lbfgs',\n",
        "            random_state=RNG\n",
        "        )\n",
        "        clf.fit(Xtr, y[tr])\n",
        "        pred = clf.predict(Xte)\n",
        "        oof_pred[te] = pred\n",
        "        f1s.append(f1_score(y[te], pred, average=\"macro\"))\n",
        "        accs.append(accuracy_score(y[te], pred))\n",
        "\n",
        "    F1m, ACC = float(np.mean(f1s)), float(np.mean(accs))\n",
        "\n",
        "    # Optional confusion matrix\n",
        "    cm = confusion_matrix(y, oof_pred)\n",
        "    labs = np.unique(y)\n",
        "    plt.figure(figsize=(4.5, 4))\n",
        "    im = plt.imshow(cm, cmap=\"Blues\")\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    plt.title(f\"{label_name} (stacked) – Confusion\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    xt = [f\"pred {i}\" for i in labs]\n",
        "    yt = [f\"true {i}\" for i in labs]\n",
        "    plt.xticks(range(len(labs)), xt, rotation=45, ha='right')\n",
        "    plt.yticks(range(len(labs)), yt)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUT / f\"{label_name}_confusion.png\", dpi=200, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"[{label_name}]  F1_macro={F1m:.3f} | Acc={ACC:.3f}\")\n",
        "    print(classification_report(y, oof_pred))\n",
        "\n",
        "    return F1m, ACC\n",
        "\n",
        "# --------- time-window permutation importance ----------\n",
        "def time_window_importance(df_sub, target_series, scorer, Yshape, n_bins=6):\n",
        "    \"\"\"\n",
        "    Permutation importance across time windows on Yshape.\n",
        "    scorer(Yshape_alt) should:\n",
        "      - perform grouped CV internally\n",
        "      - return a scalar metric (e.g. F1_macro)\n",
        "    Returns: fractional importance for S1..S6.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(0)\n",
        "    L = Yshape.shape[1]\n",
        "    bins = np.array_split(np.arange(L), n_bins)\n",
        "    base = scorer(Yshape)   # baseline with original Yshape\n",
        "    drops = []\n",
        "    for b in bins:\n",
        "        Yp = Yshape.copy()\n",
        "        Yp[:, b] = Yp[rng.permutation(len(Yp)), :][:, b]\n",
        "        s = scorer(Yp)\n",
        "        drops.append(max(0.0, base - s))\n",
        "    drops = np.asarray(drops)\n",
        "    frac = drops / (drops.sum() + 1e-12)\n",
        "    return frac\n",
        "\n",
        "# ---------- build curves & groups ----------\n",
        "ycols, Yshape, Ydiff = build_curves(dfm)\n",
        "groups = (dfm[\"article_id\"] if \"article_id\" in dfm.columns else dfm[\"curve_id\"])\\\n",
        "            .astype(\"category\").cat.codes.to_numpy()\n",
        "Xpanel = np.stack([Yshape, Ydiff], axis=1)\n",
        "Xmr_oof = oof_minirocket(Xpanel, groups)\n",
        "\n",
        "# ---------- targets ----------\n",
        "results = []\n",
        "timeimp_rows = []\n",
        "\n",
        "# 1) oil_type / material (categorical)\n",
        "for col in [\"oil_type\", \"material\"]:\n",
        "    if col in dfm.columns and dfm[col].notna().sum() >= 20:\n",
        "        y = dfm[col].astype(\"category\").cat.codes.to_numpy()\n",
        "        F1m, ACC = run_categorical(dfm, y, col, groups, Yshape, Ydiff, Xmr_oof)\n",
        "        results.append({\"target\": col, \"type\": \"categorical\", \"F1m\": F1m, \"Acc\": ACC})\n",
        "\n",
        "        # time-window importance for this categorical target\n",
        "        def scorer_cat(Yshape_alt):\n",
        "            gkf = GroupKFold(n_splits=min(5, max(3, len(np.unique(groups)))))\n",
        "            sc = []\n",
        "            for tr, te in gkf.split(Yshape_alt, groups=groups):\n",
        "                shp_tr = shape_feats(Yshape_alt[tr]).to_numpy(float)\n",
        "                shp_te = shape_feats(Yshape_alt[te]).to_numpy(float)\n",
        "                ss = StandardScaler(with_mean=True).fit(shp_tr)\n",
        "                Xtr = ss.transform(shp_tr)\n",
        "                Xte = ss.transform(shp_te)\n",
        "                clf = LogisticRegression(\n",
        "                    max_iter=3000,\n",
        "                    class_weight='balanced',\n",
        "                    multi_class='auto',\n",
        "                    solver='lbfgs',\n",
        "                    random_state=RNG\n",
        "                )\n",
        "                clf.fit(Xtr, y[tr])\n",
        "                pred = clf.predict(Xte)\n",
        "                sc.append(f1_score(y[te], pred, average=\"macro\"))\n",
        "            return float(np.mean(sc))\n",
        "\n",
        "        imp = time_window_importance(dfm, dfm[col], scorer_cat, Yshape, n_bins=6)\n",
        "        timeimp_rows.append(pd.Series(imp, index=[f\"S{i+1}\" for i in range(6)], name=col))\n",
        "\n",
        "# 2) droplet size (3-bin) if available\n",
        "if \"droplet_um\" in dfm.columns:\n",
        "    ynum = pd.to_numeric(dfm[\"droplet_um\"], errors=\"coerce\")\n",
        "    mask = ynum.notna()\n",
        "    sub = dfm.loc[mask].copy()\n",
        "    Yshape_sub = Yshape[mask]\n",
        "    Ydiff_sub  = Ydiff[mask]\n",
        "    Xmr_sub    = Xmr_oof[mask]\n",
        "    groups_sub = groups[mask]\n",
        "\n",
        "    # 3-bin global quantiles\n",
        "    qbins = pd.qcut(ynum[mask], q=3, labels=False, duplicates=\"drop\").to_numpy()\n",
        "    F1m, ACC = run_categorical(sub, qbins, \"droplet_um_3bin\", groups_sub, Yshape_sub, Ydiff_sub, Xmr_sub)\n",
        "    results.append({\"target\": \"droplet_um_3bin\", \"type\": \"categorical\", \"F1m\": F1m, \"Acc\": ACC})\n",
        "\n",
        "    # time-window importance for droplet_3bin (shape-only classifier inside scorer)\n",
        "    def scorer_drop(Yshape_alt):\n",
        "        gkf = GroupKFold(n_splits=min(5, max(3, len(np.unique(groups_sub)))))\n",
        "        sc = []\n",
        "        for tr, te in gkf.split(Yshape_alt, groups=groups_sub):\n",
        "            shp_tr = shape_feats(Yshape_alt[tr]).to_numpy(float)\n",
        "            shp_te = shape_feats(Yshape_alt[te]).to_numpy(float)\n",
        "            ss = StandardScaler(with_mean=True).fit(shp_tr)\n",
        "            Xtr = ss.transform(shp_tr)\n",
        "            Xte = ss.transform(shp_te)\n",
        "            clf = LogisticRegression(\n",
        "                max_iter=3000,\n",
        "                class_weight='balanced',\n",
        "                multi_class='auto',\n",
        "                solver='lbfgs',\n",
        "                random_state=RNG\n",
        "            )\n",
        "            clf.fit(Xtr, qbins[tr])\n",
        "            pred = clf.predict(Xte)\n",
        "            sc.append(f1_score(qbins[te], pred, average=\"macro\"))\n",
        "        return float(np.mean(sc))\n",
        "\n",
        "    imp = time_window_importance(sub, qbins, scorer_drop, Yshape_sub, n_bins=6)\n",
        "    timeimp_rows.append(pd.Series(imp, index=[f\"S{i+1}\" for i in range(6)], name=\"droplet_um_3bin\"))\n",
        "\n",
        "# 3) numeric → 3-bin (coarse classification)\n",
        "NUM_CANDS = [\"p_bar\",\"cross_flow_ms\",\"porosity\",\"pore_nm\",\"jw_lmh\",\n",
        "             \"oil_ppm\",\"salt_gl\",\"temp_c\",\"droplet_um\"]\n",
        "\n",
        "for col in NUM_CANDS:\n",
        "    if col in dfm.columns:\n",
        "        ynum = pd.to_numeric(dfm[col], errors=\"coerce\")\n",
        "        mask = ynum.notna()\n",
        "        if mask.sum() < 25:\n",
        "            continue\n",
        "        sub = dfm.loc[mask].copy()\n",
        "        Yshape_sub = Yshape[mask]\n",
        "        Ydiff_sub  = Ydiff[mask]\n",
        "        Xmr_sub    = Xmr_oof[mask]\n",
        "        groups_sub = groups[mask]\n",
        "        try:\n",
        "            ybins = pd.qcut(ynum[mask], q=3, labels=False, duplicates=\"drop\").to_numpy()\n",
        "        except Exception:\n",
        "            continue\n",
        "        F1m, ACC = run_categorical(sub, ybins, f\"{col}_3bin\", groups_sub, Yshape_sub, Ydiff_sub, Xmr_sub)\n",
        "        results.append({\"target\": col, \"type\": \"numeric→3bin\", \"F1m\": F1m, \"Acc\": ACC})\n",
        "\n",
        "        # time-window importance (shape-only inside scorer)\n",
        "        def scorer_num(Yshape_alt):\n",
        "            gkf = GroupKFold(n_splits=min(5, max(3, len(np.unique(groups_sub)))))\n",
        "            sc = []\n",
        "            for tr, te in gkf.split(Yshape_alt, groups=groups_sub):\n",
        "                shp_tr = shape_feats(Yshape_alt[tr]).to_numpy(float)\n",
        "                shp_te = shape_feats(Yshape_alt[te]).to_numpy(float)\n",
        "                ss = StandardScaler(with_mean=True).fit(shp_tr)\n",
        "                Xtr = ss.transform(shp_tr)\n",
        "                Xte = ss.transform(shp_te)\n",
        "                clf = LogisticRegression(\n",
        "                    max_iter=3000,\n",
        "                    class_weight='balanced',\n",
        "                    multi_class='auto',\n",
        "                    solver='lbfgs',\n",
        "                    random_state=RNG\n",
        "                )\n",
        "                clf.fit(Xtr, ybins[tr])\n",
        "                pred = clf.predict(Xte)\n",
        "                sc.append(f1_score(ybins[te], pred, average=\"macro\"))\n",
        "            return float(np.mean(sc))\n",
        "\n",
        "        imp = time_window_importance(sub, ybins, scorer_num, Yshape_sub, n_bins=6)\n",
        "        timeimp_rows.append(pd.Series(imp, index=[f\"S{i+1}\" for i in range(6)], name=f\"{col}_3bin\"))\n",
        "\n",
        "# ----- save tables -----\n",
        "res_df = pd.DataFrame(results)\n",
        "if not res_df.empty:\n",
        "    res_df = res_df[[\"target\",\"type\",\"F1m\",\"Acc\"]]\n",
        "res_df.to_csv(OUT / \"curve_to_meta_results.csv\", index=False)\n",
        "\n",
        "if timeimp_rows:\n",
        "    imp_df = pd.DataFrame(timeimp_rows)\n",
        "    imp_df.to_csv(OUT / \"time_window_importance.csv\")\n",
        "    # quick bar for one exemplar (oil_type or droplet)\n",
        "    pick = [n for n in imp_df.index if \"oil_type\" in n or \"droplet\" in n]\n",
        "    if pick:\n",
        "        row = imp_df.loc[pick[0]]\n",
        "        plt.figure(figsize=(5.5,3))\n",
        "        row.plot(kind=\"bar\")\n",
        "        plt.title(f\"Time-window importance: {pick[0]} (S1..S6)\")\n",
        "        plt.ylabel(\"fraction\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(OUT / f\"time_importance_{pick[0]}.png\", dpi=200, bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "\n",
        "# ----- zip everything -----\n",
        "zip_path = shutil.make_archive(\"pyts_final\", \"zip\", base_dir=str(OUT))\n",
        "print(\"\\nSaved:\")\n",
        "print(\" -\", (OUT / \"curve_to_meta_results.csv\").resolve())\n",
        "if timeimp_rows:\n",
        "    print(\" -\", (OUT / \"time_window_importance.csv\").resolve())\n",
        "print(\" -\", os.path.abspath(zip_path))\n",
        "print(\"\\nResults:\")\n",
        "print(res_df if not res_df.empty else \"(no targets available)\")\n"
      ]
    }
  ]
}