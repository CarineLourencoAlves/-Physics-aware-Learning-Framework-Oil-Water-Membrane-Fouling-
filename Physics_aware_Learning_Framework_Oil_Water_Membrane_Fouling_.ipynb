{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdLjNQxYUtbU"
      },
      "outputs": [],
      "source": [
        "# === CLEAN TWO-STAGE CURVE MODELING PIPELINE ===\n",
        "# Uses:\n",
        "#   - Long_Normalized_All.xlsx : long format flux–time curves (possibly many sheets)\n",
        "#   - CurveMeta_All.csv        : curve-level metadata (one row per article/curve/condition)\n",
        "#\n",
        "# Stage A: fit KWW to each curve -> J0, Jinf, tau, beta\n",
        "# Stage B: metadata -> parameters models with GroupKFold (grouped by article_id)\n",
        "# Stage C: reconstruct curves & compute curve-level MAE\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.optimize import curve_fit\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.model_selection import GroupKFold, KFold, cross_val_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 0) Paths and small helpers\n",
        "# -------------------------------------------------------------------\n",
        "LONG_XLSX = \"Long_Normalized_All.xlsx\"\n",
        "META_CSV  = \"CurveMeta_All.csv\"\n",
        "\n",
        "def norm_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Strip, replace whitespace by '_', and lowercase all column names.\"\"\"\n",
        "    df = df.copy()\n",
        "    df.columns = (df.columns\n",
        "                  .str.strip()\n",
        "                  .str.replace(r\"\\s+\", \"_\", regex=True)\n",
        "                  .str.lower())\n",
        "    return df\n",
        "\n",
        "def norm_id_series(s: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Normalize ID columns:\n",
        "    - strip, lowercase\n",
        "    - try to interpret as numeric -> integer -> string (so '5.0' -> '5')\n",
        "    - collapse spaces to underscores\n",
        "    \"\"\"\n",
        "    s = s.astype(str).str.strip().str.lower()\n",
        "    asnum = pd.to_numeric(s, errors=\"coerce\")\n",
        "    s_num = asnum.astype(\"Int64\").astype(str)  # '5' instead of '5.0'\n",
        "    s = s.where(asnum.isna(), s_num)\n",
        "    s = s.str.replace(r\"\\s+\", \" \", regex=True).str.replace(\" \", \"_\")\n",
        "    return s\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1) Load data\n",
        "# -------------------------------------------------------------------\n",
        "if not os.path.exists(LONG_XLSX):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Time-series workbook '{LONG_XLSX}' not found. \"\n",
        "        \"Upload it or adjust LONG_XLSX.\"\n",
        "    )\n",
        "if not os.path.exists(META_CSV):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Metadata file '{META_CSV}' not found. \"\n",
        "        \"Upload it or adjust META_CSV.\"\n",
        "    )\n",
        "\n",
        "# Load all sheets from the long workbook and attach article_id if missing\n",
        "sheets = pd.read_excel(LONG_XLSX, sheet_name=None)\n",
        "\n",
        "long_parts = []\n",
        "for sheet_name, df_sheet in sheets.items():\n",
        "    df_sheet = df_sheet.copy()\n",
        "    if \"article_id\" not in df_sheet.columns:\n",
        "        df_sheet[\"article_id\"] = sheet_name\n",
        "    long_parts.append(df_sheet)\n",
        "\n",
        "long = norm_cols(pd.concat(long_parts, ignore_index=True))\n",
        "meta = norm_cols(pd.read_csv(META_CSV))\n",
        "\n",
        "# Normalize ID columns (article_id, curve_id, conditionid)\n",
        "for df_ in (long, meta):\n",
        "    for c in [\"article_id\", \"curve_id\", \"conditionid\"]:\n",
        "        if c in df_.columns:\n",
        "            df_[c] = norm_id_series(df_[c])\n",
        "\n",
        "print(f\"Loaded LONG rows: {len(long)}\")\n",
        "print(f\"Loaded META rows: {len(meta)}\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2) Time & target columns (NormFlux)\n",
        "# -------------------------------------------------------------------\n",
        "y_col = \"normflux\"\n",
        "\n",
        "cand_min = [\"t_min\", \"time_min\", \"minutes\", \"min\"]\n",
        "cand_sec = [\"t_s\", \"t_sec\", \"time_s\", \"seconds\", \"sec\", \"t\", \"time\"]\n",
        "\n",
        "# auto-detect time column\n",
        "t_col = next((c for c in cand_min if c in long.columns), None)\n",
        "if t_col is None:\n",
        "    for c in cand_sec:\n",
        "        if c in long.columns:\n",
        "            long[c] = pd.to_numeric(long[c], errors=\"coerce\")\n",
        "            long[\"t_min\"] = long[c] / 60.0\n",
        "            t_col = \"t_min\"\n",
        "            break\n",
        "\n",
        "if t_col is None:\n",
        "    raise KeyError(\n",
        "        \"No time column found in LONG. \"\n",
        "        f\"Expected one of {cand_min + cand_sec}, got {long.columns.tolist()}\"\n",
        "    )\n",
        "\n",
        "# numeric coercion\n",
        "long[y_col] = pd.to_numeric(long[y_col], errors=\"coerce\")\n",
        "long[t_col] = pd.to_numeric(long[t_col], errors=\"coerce\")\n",
        "\n",
        "# per-curve ID key (triple)\n",
        "id_cols = [\"article_id\", \"curve_id\", \"conditionid\"]\n",
        "need_cols = id_cols + [y_col, t_col]\n",
        "\n",
        "long_use = long.dropna(subset=need_cols).copy()\n",
        "print(\"Usable long rows (no NaN in ID/time/flux):\", len(long_use))\n",
        "\n",
        "# Data-driven upper bound for flux (prevents crazy 1.5 plateaus)\n",
        "Y_UB = float(1.2 * np.nanpercentile(long_use[y_col], 99))\n",
        "if not np.isfinite(Y_UB) or Y_UB <= 0:\n",
        "    Y_UB = 1.5  # fallback\n",
        "print(f\"Flux upper bound (Y_UB): {Y_UB:.3f}\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3) Stage A: per-curve KWW fits\n",
        "# -------------------------------------------------------------------\n",
        "def kww(t, J0, Jinf, tau, beta):\n",
        "    \"\"\"\n",
        "    Stretched-exponential (KWW):\n",
        "        J(t) = Jinf + (J0 - Jinf) * exp( - (t/tau)^beta )\n",
        "    \"\"\"\n",
        "    t = np.asarray(t, float)\n",
        "    tau = max(float(tau), 1e-9)\n",
        "    beta = float(beta)\n",
        "    return Jinf + (J0 - Jinf) * np.exp(-np.power(np.clip(t, 0, None) / tau, beta))\n",
        "\n",
        "def fit_one_curve(t, y):\n",
        "    \"\"\"Fit KWW to one curve, return dict with params and R².\"\"\"\n",
        "    t = np.asarray(t, float)\n",
        "    y = np.asarray(y, float)\n",
        "    if len(t) < 3:\n",
        "        return None\n",
        "\n",
        "    # Initial guesses\n",
        "    J0_guess   = float(np.nanmax(y))\n",
        "    Jinf_guess = float(np.nanmin(y))\n",
        "    tau_guess  = max(float(np.nanmedian(t)), 1e-3)\n",
        "    beta_guess = 1.0\n",
        "    p0 = [J0_guess, Jinf_guess, tau_guess, beta_guess]\n",
        "\n",
        "    # Bounds based on data-driven Y_UB\n",
        "    bounds = ([0.0, 0.0, 1e-6, 0.05],\n",
        "              [Y_UB, Y_UB, 1e6, 3.0])\n",
        "\n",
        "    try:\n",
        "        popt, _ = curve_fit(kww, t, y, p0=p0, bounds=bounds, maxfev=20000)\n",
        "    except Exception:\n",
        "        popt = p0\n",
        "\n",
        "    yhat = kww(t, *popt)\n",
        "    ss_res = np.sum((y - yhat)**2)\n",
        "    ss_tot = np.sum((y - np.mean(y))**2) + 1e-12\n",
        "    r2 = 1.0 - ss_res / ss_tot\n",
        "\n",
        "    return {\n",
        "        \"J0\":   float(popt[0]),\n",
        "        \"Jinf\": float(popt[1]),\n",
        "        \"tau\":  float(popt[2]),\n",
        "        \"beta\": float(popt[3]),\n",
        "        \"fit_r2\": float(r2),\n",
        "    }\n",
        "\n",
        "MIN_POINTS = 5  # minimum points per curve to attempt a fit\n",
        "\n",
        "fit_rows = []\n",
        "for keys, g in long_use.groupby(id_cols):\n",
        "    g = g.sort_values(t_col)\n",
        "    if len(g) < MIN_POINTS:\n",
        "        continue\n",
        "    res = fit_one_curve(g[t_col].values, g[y_col].values)\n",
        "    if res is None:\n",
        "        continue\n",
        "    rec = dict(zip(id_cols, keys))\n",
        "    rec.update(res)\n",
        "    fit_rows.append(rec)\n",
        "\n",
        "params_df = pd.DataFrame(fit_rows)\n",
        "print(\"Fitted curves:\", len(params_df))\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 4) Join KWW params with META (triple key)\n",
        "# -------------------------------------------------------------------\n",
        "num_feats = [\"p_bar\", \"oil_ppm\", \"salt_gl\", \"temp_c\",\n",
        "             \"porosity\", \"pore_nm\", \"droplet_um\"]\n",
        "cat_feats = [\"material\", \"geometry\", \"oil_type\"]\n",
        "\n",
        "# numeric coercion in META\n",
        "for c in num_feats:\n",
        "    if c in meta.columns:\n",
        "        meta[c] = pd.to_numeric(meta[c], errors=\"coerce\")\n",
        "\n",
        "keep_cols = id_cols + [c for c in num_feats + cat_feats if c in meta.columns]\n",
        "\n",
        "# one row per (article_id, curve_id, conditionid): first non-null value\n",
        "meta_one = (\n",
        "    meta[keep_cols]\n",
        "    .groupby(id_cols, as_index=False)\n",
        "    .agg({c: (lambda s: s.dropna().iloc[0] if s.dropna().size else np.nan)\n",
        "          for c in keep_cols if c not in id_cols})\n",
        ")\n",
        "\n",
        "meta_full = params_df.merge(meta_one, on=id_cols, how=\"inner\")\n",
        "print(\"Rows after join (params + metadata):\", len(meta_full))\n",
        "\n",
        "if meta_full.empty:\n",
        "    raise RuntimeError(\n",
        "        \"No overlap between LONG and META on (article_id, curve_id, conditionid). \"\n",
        "        \"Check ID formatting in your Excel/CSV.\"\n",
        "    )\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 5) Stage B: metadata -> parameter models\n",
        "# -------------------------------------------------------------------\n",
        "present_num = [c for c in num_feats if c in meta_full.columns]\n",
        "present_cat = [c for c in cat_feats if c in meta_full.columns]\n",
        "\n",
        "X = meta_full[present_num + present_cat].copy()\n",
        "meta_full[\"log_tau\"] = np.log(meta_full[\"tau\"].clip(lower=1e-6))\n",
        "\n",
        "targets = {\n",
        "    \"J0\":      meta_full[\"J0\"].astype(float).values,\n",
        "    \"Jinf\":    meta_full[\"Jinf\"].astype(float).values,\n",
        "    \"log_tau\": meta_full[\"log_tau\"].astype(float).values,\n",
        "    \"beta\":    meta_full[\"beta\"].astype(float).values,\n",
        "}\n",
        "\n",
        "num_pipe = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"sc\",  StandardScaler())\n",
        "])\n",
        "cat_pipe = Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"oh\",  OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "pre = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", num_pipe, present_num),\n",
        "        (\"cat\", cat_pipe, present_cat),\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "reg = HistGradientBoostingRegressor(\n",
        "    learning_rate=0.06,\n",
        "    max_leaf_nodes=31,\n",
        "    min_samples_leaf=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "def make_cv(n_rows: int, groups: pd.Series | None):\n",
        "    \"\"\"Build GroupKFold by article_id if possible, else plain KFold.\"\"\"\n",
        "    if n_rows < 2:\n",
        "        return None, {}\n",
        "    if groups is not None and groups.nunique() >= 2:\n",
        "        n_splits = min(5, groups.nunique(), n_rows)\n",
        "        return GroupKFold(n_splits=n_splits), {\"groups\": groups}\n",
        "    # fallback: plain KFold\n",
        "    from sklearn.model_selection import KFold\n",
        "    n_splits = min(5, max(2, min(3, n_rows)))\n",
        "    return KFold(n_splits=n_splits, shuffle=True, random_state=42), {}\n",
        "\n",
        "def eval_target(y_vec: np.ndarray):\n",
        "    \"\"\"Cross-validate and fit one parameter model.\"\"\"\n",
        "    mask = np.isfinite(y_vec)\n",
        "    Xy = X.loc[mask]\n",
        "    yy = y_vec[mask]\n",
        "    n = len(Xy)\n",
        "    pipe = Pipeline([(\"pre\", pre), (\"reg\", reg)])\n",
        "\n",
        "    if n < 2:\n",
        "        model = pipe.fit(Xy, yy) if n >= 1 else pipe.fit(X, y_vec)\n",
        "        return np.nan, np.nan, model\n",
        "\n",
        "    groups = meta_full.loc[mask, \"article_id\"].astype(str) if \"article_id\" in meta_full.columns else None\n",
        "    cv, kwargs = make_cv(n, groups)\n",
        "\n",
        "    if cv is None:\n",
        "        model = pipe.fit(Xy, yy)\n",
        "        return np.nan, np.nan, model\n",
        "\n",
        "    mae = -cross_val_score(pipe, Xy, yy, cv=cv,\n",
        "                           scoring=\"neg_mean_absolute_error\",\n",
        "                           n_jobs=-1, **kwargs).mean()\n",
        "    rmse = -cross_val_score(pipe, Xy, yy, cv=cv,\n",
        "                            scoring=\"neg_root_mean_squared_error\",\n",
        "                            n_jobs=-1, **kwargs).mean()\n",
        "    model = pipe.fit(Xy, yy)\n",
        "    return mae, rmse, model\n",
        "\n",
        "models = {}\n",
        "print(\"\\nCV (per-parameter):\")\n",
        "for name, y_vec in targets.items():\n",
        "    mae, rmse, model = eval_target(y_vec)\n",
        "    models[name] = model\n",
        "    mae_s  = f\"{mae:.4f}\"  if np.isfinite(mae)  else \"—\"\n",
        "    rmse_s = f\"{rmse:.4f}\" if np.isfinite(rmse) else \"—\"\n",
        "    print(f\"  {name:>7s}  MAE={mae_s}  RMSE={rmse_s}\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 6) Reconstruct some curves from predicted parameters\n",
        "# -------------------------------------------------------------------\n",
        "def predict_params_for_row(xrow: pd.Series) -> dict:\n",
        "    \"\"\"Predict J0, Jinf, tau, beta from one metadata row.\"\"\"\n",
        "    xdf = pd.DataFrame([xrow], columns=X.columns)\n",
        "    J0p   = float(np.clip(models[\"J0\"].predict(xdf)[0],   0.0, Y_UB))\n",
        "    Jinfp = float(np.clip(models[\"Jinf\"].predict(xdf)[0], 0.0, Y_UB))\n",
        "    taup  = float(np.exp(models[\"log_tau\"].predict(xdf)[0]))\n",
        "    betap = float(np.clip(models[\"beta\"].predict(xdf)[0], 0.05, 3.0))\n",
        "    return {\"J0\": J0p, \"Jinf\": Jinfp, \"tau\": max(taup, 1e-6), \"beta\": betap}\n",
        "\n",
        "# Plot a few random curves\n",
        "example_rows = meta_full[id_cols].sample(min(4, len(meta_full)), random_state=42)\n",
        "fig, axes = plt.subplots(len(example_rows), 1, figsize=(6, 3*len(example_rows)))\n",
        "if len(example_rows) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for ax, (_, rec) in zip(axes, example_rows.iterrows()):\n",
        "    a, c, k = rec[\"article_id\"], rec[\"curve_id\"], rec[\"conditionid\"]\n",
        "\n",
        "    g = (\n",
        "        long_use[\n",
        "            (long_use[\"article_id\"] == a) &\n",
        "            (long_use[\"curve_id\"]   == c) &\n",
        "            (long_use[\"conditionid\"] == k)\n",
        "        ][[t_col, y_col]]\n",
        "        .dropna()\n",
        "        .sort_values(t_col)\n",
        "    )\n",
        "    if g.empty:\n",
        "        continue\n",
        "\n",
        "    xr = (\n",
        "        meta_full[\n",
        "            (meta_full[\"article_id\"] == a) &\n",
        "            (meta_full[\"curve_id\"]   == c) &\n",
        "            (meta_full[\"conditionid\"] == k)\n",
        "        ][present_num + present_cat]\n",
        "        .iloc[0]\n",
        "    )\n",
        "\n",
        "    p = predict_params_for_row(xr)\n",
        "\n",
        "    tt = np.linspace(float(g[t_col].min()), float(g[t_col].max()), 200)\n",
        "    yp = kww(tt, p[\"J0\"], p[\"Jinf\"], p[\"tau\"], p[\"beta\"])\n",
        "\n",
        "    ax.plot(g[t_col], g[y_col], \"o\", ms=4, lw=1, label=\"true\")\n",
        "    ax.plot(tt, yp, lw=2, label=\"pred\")\n",
        "    ax.set_title(\n",
        "        f\"{c} | cond={k} | \"\n",
        "        f\"J0={p['J0']:.3f}, Jinf={p['Jinf']:.3f}, tau={p['tau']:.3g}, beta={p['beta']:.2f}\"\n",
        "    )\n",
        "    ax.set_xlabel(\"t_min\")\n",
        "    ax.set_ylabel(\"NormFlux\")\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 7) Curve-level MAE (in-sample, using predicted params)\n",
        "# -------------------------------------------------------------------\n",
        "def curve_mae_triple(a, c, k):\n",
        "    g = (\n",
        "        long_use[\n",
        "            (long_use[\"article_id\"] == a) &\n",
        "            (long_use[\"curve_id\"]   == c) &\n",
        "            (long_use[\"conditionid\"] == k)\n",
        "        ][[t_col, y_col]]\n",
        "        .dropna()\n",
        "        .sort_values(t_col)\n",
        "    )\n",
        "    if g.empty:\n",
        "        return np.nan\n",
        "\n",
        "    xr = (\n",
        "        meta_full[\n",
        "            (meta_full[\"article_id\"] == a) &\n",
        "            (meta_full[\"curve_id\"]   == c) &\n",
        "            (meta_full[\"conditionid\"] == k)\n",
        "        ][present_num + present_cat]\n",
        "        .iloc[0]\n",
        "    )\n",
        "    p = predict_params_for_row(xr)\n",
        "    y_true = g[y_col].values\n",
        "    y_pred = kww(g[t_col].values, p[\"J0\"], p[\"Jinf\"], p[\"tau\"], p[\"beta\"])\n",
        "    return mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "curve_errs = [\n",
        "    curve_mae_triple(r[\"article_id\"], r[\"curve_id\"], r[\"conditionid\"])\n",
        "    for _, r in meta_full[id_cols].iterrows()\n",
        "]\n",
        "curve_errs = pd.Series(curve_errs, name=\"curve_MAE\")\n",
        "print(\"\\nCurve-level reconstruction MAE (in-sample, using predicted params):\")\n",
        "print(curve_errs.describe())\n"
      ]
    }
  ]
}